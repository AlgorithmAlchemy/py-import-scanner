"""
–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤ - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
"""
import os
import ast
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict

from .logging_config import get_logger
from .import_parser import ImportParser
from .dependency_analyzer import DependencyAnalyzer
from .complexity_analyzer import ComplexityAnalyzer
from .code_quality_analyzer import CodeQualityAnalyzer
from .architecture_analyzer import ArchitectureAnalyzer


@dataclass
class ProjectStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
    total_files: int = 0
    total_lines: int = 0
    total_imports: int = 0
    unique_libraries: int = 0
    average_complexity: float = 0.0
    quality_score: float = 0.0
    architecture_score: float = 0.0
    scan_duration: float = 0.0


@dataclass
class LibraryInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ"""
    name: str
    count: int
    percentage: float
    files: List[str]
    first_occurrence: str


@dataclass
class FileAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞"""
    path: str
    lines: int
    imports: List[str]
    complexity: float
    quality_score: float
    issues: List[str]


class IntegratedProjectAnalyzer:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤"""
    
    def __init__(self, config):
        self.config = config
        self.logger = get_logger("IntegratedProjectAnalyzer")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.import_parser = ImportParser(config)
        self.dependency_analyzer = DependencyAnalyzer()
        self.complexity_analyzer = ComplexityAnalyzer()
        self.quality_analyzer = CodeQualityAnalyzer()
        self.architecture_analyzer = ArchitectureAnalyzer()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.project_stats = ProjectStats()
        self.libraries_info: Dict[str, LibraryInfo] = {}
        self.files_analysis: List[FileAnalysis] = []
        self.dependency_graph: Dict[str, List[str]] = {}
        self.architecture_data: Dict[str, Any] = {}
        
    def analyze_project(self, project_path: Path, progress_callback=None) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞
        
        Args:
            project_path: –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
            progress_callback: –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        start_time = time.time()
        self.logger.info(f"–ù–∞—á–∞–ª–æ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞: {project_path}")
        
        if progress_callback:
            progress_callback("üîç –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞...")
        
        try:
            # 1. –ü–æ–∏—Å–∫ Python —Ñ–∞–π–ª–æ–≤
            python_files = self._find_python_files(project_path)
            self.project_stats.total_files = len(python_files)
            
            if progress_callback:
                progress_callback(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(python_files)} Python —Ñ–∞–π–ª–æ–≤")
            
            # 2. –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤
            if progress_callback:
                progress_callback("üì¶ –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤...")
            
            imports_data = self._analyze_imports(python_files, progress_callback)
            
            # 3. –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            if progress_callback:
                progress_callback("üìä –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞...")
            
            complexity_data = self._analyze_complexity(python_files, progress_callback)
            
            # 4. –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            if progress_callback:
                progress_callback("‚ú® –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞...")
            
            quality_data = self._analyze_quality(python_files, progress_callback)
            
            # 5. –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
            if progress_callback:
                progress_callback("üèóÔ∏è –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã...")
            
            architecture_data = self._analyze_architecture(project_path, progress_callback)
            
            # 6. –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            if progress_callback:
                progress_callback("üîó –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
            
            dependency_data = self._analyze_dependencies(project_path, progress_callback)
            
            # 7. –°–±–æ—Ä–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if progress_callback:
                progress_callback("üìà –°–±–æ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
            
            self._build_final_stats(imports_data, complexity_data, quality_data)
            
            # 8. –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
            self.project_stats.scan_duration = time.time() - start_time
            
            if progress_callback:
                progress_callback("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
            return self._generate_comprehensive_report()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–æ–µ–∫—Ç–∞: {e}")
            if progress_callback:
                progress_callback(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            raise
    
    def _find_python_files(self, project_path: Path) -> List[Path]:
        """–ü–æ–∏—Å–∫ –≤—Å–µ—Ö Python —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
        python_files = []
        excluded_dirs = {'.git', '__pycache__', '.pytest_cache', 'venv', 'env', 'node_modules'}
        
        for root, dirs, files in os.walk(project_path):
            # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [d for d in dirs if d not in excluded_dirs]
            
            for file in files:
                if file.endswith('.py'):
                    python_files.append(Path(root) / file)
        
        return python_files
    
    def _analyze_imports(self, python_files: List[Path], progress_callback=None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤ –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö"""
        all_imports = []
        file_imports = {}
        
        for i, file_path in enumerate(python_files):
            if progress_callback and i % 10 == 0:
                progress_callback(f"üì¶ –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤: {i+1}/{len(python_files)}")
            
            try:
                imports = self.import_parser.parse_imports(
                    file_path.read_text(encoding='utf-8', errors='ignore'),
                    file_path
                )
                all_imports.extend(imports)
                file_imports[str(file_path)] = imports
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–º–ø–æ—Ä—Ç–æ–≤ –≤ {file_path}: {e}")
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        import_counter = Counter(all_imports)
        total_imports = len(all_imports)
        unique_libraries = len(import_counter)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö
        for lib_name, count in import_counter.most_common():
            percentage = (count / total_imports * 100) if total_imports > 0 else 0
            files_using_lib = [f for f, imports in file_imports.items() if lib_name in imports]
            
            self.libraries_info[lib_name] = LibraryInfo(
                name=lib_name,
                count=count,
                percentage=percentage,
                files=files_using_lib,
                first_occurrence=files_using_lib[0] if files_using_lib else ""
            )
        
        return {
            'total_imports': total_imports,
            'unique_libraries': unique_libraries,
            'libraries_info': self.libraries_info,
            'file_imports': file_imports
        }
    
    def _analyze_complexity(self, python_files: List[Path], progress_callback=None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞"""
        complexity_scores = []
        
        for i, file_path in enumerate(python_files):
            if progress_callback and i % 10 == 0:
                progress_callback(f"üìä –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {i+1}/{len(python_files)}")
            
            try:
                content = file_path.read_text(encoding='utf-8', errors='ignore')
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ analyze_file
                complexity_report = self.complexity_analyzer.analyze_file(file_path)
                complexity = complexity_report.metrics.cyclomatic_complexity
                complexity_scores.append(complexity)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
                file_analysis = FileAnalysis(
                    path=str(file_path),
                    lines=len(content.splitlines()),
                    imports=[],  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                    complexity=complexity,
                    quality_score=0.0,  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                    issues=[]
                )
                self.files_analysis.append(file_analysis)
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ {file_path}: {e}")
        
        avg_complexity = sum(complexity_scores) / len(complexity_scores) if complexity_scores else 0
        
        return {
            'average_complexity': avg_complexity,
            'complexity_scores': complexity_scores,
            'max_complexity': max(complexity_scores) if complexity_scores else 0,
            'min_complexity': min(complexity_scores) if complexity_scores else 0
        }
    
    def _analyze_quality(self, python_files: List[Path], progress_callback=None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞"""
        quality_scores = []
        
        for i, file_path in enumerate(python_files):
            if progress_callback and i % 10 == 0:
                progress_callback(f"‚ú® –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞: {i+1}/{len(python_files)}")
            
            try:
                content = file_path.read_text(encoding='utf-8', errors='ignore')
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ analyze_file
                quality_report = self.quality_analyzer.analyze_file(file_path)
                quality_score = quality_report.overall_score
                quality_scores.append(quality_score)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
                for file_analysis in self.files_analysis:
                    if file_analysis.path == str(file_path):
                        file_analysis.quality_score = quality_score
                        file_analysis.issues = quality_report.issues if hasattr(quality_report, 'issues') else []
                        break
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ {file_path}: {e}")
        
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        return {
            'average_quality': avg_quality,
            'quality_scores': quality_scores,
            'quality_distribution': self._calculate_quality_distribution(quality_scores)
        }
    
    def _analyze_architecture(self, project_path: Path, progress_callback=None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
        if progress_callback:
            progress_callback("üèóÔ∏è –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞...")
        
        try:
            architecture_result = self.architecture_analyzer.analyze_project(project_path)
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å
            self.architecture_data = {
                'modules': len(architecture_result.modules) if hasattr(architecture_result, 'modules') else 0,
                'dependencies': len(architecture_result.dependencies) if hasattr(architecture_result, 'dependencies') else 0,
                'patterns': architecture_result.patterns if hasattr(architecture_result, 'patterns') else [],
                'recommendations': architecture_result.recommendations if hasattr(architecture_result, 'recommendations') else []
            }
            return self.architecture_data
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {e}")
            return {}
    
    def _analyze_dependencies(self, project_path: Path, progress_callback=None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø—Ä–æ–µ–∫—Ç–∞"""
        if progress_callback:
            progress_callback("üîó –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–ø–æ—Ä—Ç–æ–≤
            dependency_graph = {}
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–º–ø–æ—Ä—Ç—ã –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ
            for file_analysis in self.files_analysis:
                file_path = Path(file_analysis.path)
                module_name = file_path.stem
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
                try:
                    imports = self.import_parser.parse_imports(
                        file_path.read_text(encoding='utf-8', errors='ignore'),
                        file_path
                    )
                    dependency_graph[module_name] = list(set(imports))
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ {file_path}: {e}")
                    dependency_graph[module_name] = []
            
            self.dependency_graph = dependency_graph
            
            return {
                'dependency_graph': dependency_graph,
                'total_modules': len(dependency_graph),
                'total_dependencies': sum(len(deps) for deps in dependency_graph.values())
            }
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {e}")
            return {}
    
    def _build_final_stats(self, imports_data, complexity_data, quality_data):
        """–°–±–æ—Ä–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.project_stats.total_imports = imports_data['total_imports']
        self.project_stats.unique_libraries = imports_data['unique_libraries']
        self.project_stats.average_complexity = complexity_data['average_complexity']
        self.project_stats.quality_score = quality_data['average_quality']
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
        total_lines = sum(file_analysis.lines for file_analysis in self.files_analysis)
        self.project_stats.total_lines = total_lines
    
    def _calculate_quality_distribution(self, quality_scores: List[float]) -> Dict[str, int]:
        """–†–∞—Å—á–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"""
        distribution = {
            'excellent': 0,  # 90-100
            'good': 0,       # 70-89
            'fair': 0,       # 50-69
            'poor': 0,       # 30-49
            'very_poor': 0   # 0-29
        }
        
        for score in quality_scores:
            if score >= 90:
                distribution['excellent'] += 1
            elif score >= 70:
                distribution['good'] += 1
            elif score >= 50:
                distribution['fair'] += 1
            elif score >= 30:
                distribution['poor'] += 1
            else:
                distribution['very_poor'] += 1
        
        return distribution
    
    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        return {
            'project_stats': asdict(self.project_stats),
            'libraries_info': {name: asdict(info) for name, info in self.libraries_info.items()},
            'files_analysis': [asdict(analysis) for analysis in self.files_analysis],
            'dependency_graph': self.dependency_graph,
            'architecture_data': self.architecture_data,
            'top_libraries': self._get_top_libraries(10),
            'quality_distribution': self._calculate_quality_distribution(
                [f.quality_score for f in self.files_analysis]
            ),
            'complexity_distribution': self._calculate_complexity_distribution(),
            'scan_timestamp': time.time()
        }
    
    def _get_top_libraries(self, count: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø –±–∏–±–ª–∏–æ—Ç–µ–∫"""
        sorted_libraries = sorted(
            self.libraries_info.values(),
            key=lambda x: x.count,
            reverse=True
        )
        
        return [asdict(lib) for lib in sorted_libraries[:count]]
    
    def _calculate_complexity_distribution(self) -> Dict[str, int]:
        """–†–∞—Å—á–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        distribution = {
            'very_low': 0,   # 0-5
            'low': 0,        # 6-10
            'medium': 0,     # 11-20
            'high': 0,       # 21-30
            'very_high': 0   # 30+
        }
        
        for file_analysis in self.files_analysis:
            complexity = file_analysis.complexity
            if complexity <= 5:
                distribution['very_low'] += 1
            elif complexity <= 10:
                distribution['low'] += 1
            elif complexity <= 20:
                distribution['medium'] += 1
            elif complexity <= 30:
                distribution['high'] += 1
            else:
                distribution['very_high'] += 1
        
        return distribution
    
    def export_report(self, output_path: Path, format: str = 'json') -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        report = self._generate_comprehensive_report()
        
        if format.lower() == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
        elif format.lower() == 'txt':
            self._export_text_report(output_path, report)
        
        self.logger.info(f"–û—Ç—á–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {output_path}")
    
    def _export_text_report(self, output_path: Path, report: Dict[str, Any]) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("–û–¢–ß–ï–¢ –û–ë –ê–ù–ê–õ–ò–ó–ï –ü–†–û–ï–ö–¢–ê\n")
            f.write("=" * 60 + "\n\n")
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = report['project_stats']
            f.write("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
            f.write(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {stats['total_files']}\n")
            f.write(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {stats['total_lines']}\n")
            f.write(f"   –í—Å–µ–≥–æ –∏–º–ø–æ—Ä—Ç–æ–≤: {stats['total_imports']}\n")
            f.write(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫: {stats['unique_libraries']}\n")
            f.write(f"   –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {stats['average_complexity']:.2f}\n")
            f.write(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {stats['quality_score']:.2f}\n")
            f.write(f"   –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {stats['scan_duration']:.2f}—Å\n\n")
            
            # –¢–æ–ø –±–∏–±–ª–∏–æ—Ç–µ–∫
            f.write("üèÜ –¢–û–ü-10 –ë–ò–ë–õ–ò–û–¢–ï–ö:\n")
            for i, lib in enumerate(report['top_libraries'], 1):
                f.write(f"   {i:2d}. {lib['name']:20s} - {lib['count']:4d} ({lib['percentage']:5.1f}%)\n")
            f.write("\n")
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞
            f.write("‚ú® –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–ß–ï–°–¢–í–ê:\n")
            quality_dist = report['quality_distribution']
            f.write(f"   –û—Ç–ª–∏—á–Ω–æ–µ (90-100): {quality_dist['excellent']}\n")
            f.write(f"   –•–æ—Ä–æ—à–µ–µ (70-89): {quality_dist['good']}\n")
            f.write(f"   –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ (50-69): {quality_dist['fair']}\n")
            f.write(f"   –ü–ª–æ—Ö–æ–µ (30-49): {quality_dist['poor']}\n")
            f.write(f"   –û—á–µ–Ω—å –ø–ª–æ—Ö–æ–µ (0-29): {quality_dist['very_poor']}\n\n")
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            f.write("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–õ–û–ñ–ù–û–°–¢–ò:\n")
            complexity_dist = report['complexity_distribution']
            f.write(f"   –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è (0-5): {complexity_dist['very_low']}\n")
            f.write(f"   –ù–∏–∑–∫–∞—è (6-10): {complexity_dist['low']}\n")
            f.write(f"   –°—Ä–µ–¥–Ω—è—è (11-20): {complexity_dist['medium']}\n")
            f.write(f"   –í—ã—Å–æ–∫–∞—è (21-30): {complexity_dist['high']}\n")
            f.write(f"   –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (30+): {complexity_dist['very_high']}\n\n")
